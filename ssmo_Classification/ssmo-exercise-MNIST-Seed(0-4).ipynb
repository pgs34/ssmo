{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e895f5c-2910-44d0-aac2-d9afb0f33b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47ffec2-8590-4af0-a35e-d3b98f8049e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cuDNN (중요)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # PyTorch 2.x (있으면 더 강력)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "def subsample_dataset(dataset, num_samples, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    indices = torch.randperm(len(dataset))[:num_samples]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "def get_dataloaders(train_dataset, test_dataset, seed):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        generator=g,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc58c71-5adc-4a94-9cda-9b44dd8a048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSAMPLE_SEED = 42\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 5,000개만 사용\n",
    "train_dataset_small = subsample_dataset(\n",
    "    full_train_dataset,\n",
    "    num_samples=5000,\n",
    "    seed=SUBSAMPLE_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4c2bf5-6903-4a64-b205-bfc3c1737ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666d733b-cffe-40be-8b16-962a2d056246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),   # 32 → 8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(8, 16, 3, padding=1), # 64 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(16 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MLP_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),  # 512 → 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),       # 256 → 64\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44452b90-ee92-4cb1-85fc-e0460958cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6681f868-1f3f-4a68-91e5-e81eb59b7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "def train_one_epoch(model, optimizer, criterion, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, T=2.0):\n",
    "    return kl_loss(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    ) * (T * T)\n",
    "\n",
    "def studygroup(\n",
    "    model1,\n",
    "    model2,\n",
    "    optimizer1,\n",
    "    optimizer2,\n",
    "    train_loader,\n",
    "    device,\n",
    "    T=2.0\n",
    "):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "\n",
    "    train_loss1 = 0.0\n",
    "    train_loss2 = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        logits1 = model1(x)\n",
    "        logits2 = model2(x)\n",
    "\n",
    "        pred1 = logits1.argmax(dim=1)\n",
    "        pred2 = logits2.argmax(dim=1)\n",
    "\n",
    "        correct1 = pred1 == y\n",
    "        correct2 = pred2 == y\n",
    "\n",
    "        # index 정의\n",
    "        index1 = correct1 & ~correct2   # model1 \n",
    "        index2 = ~correct1 & correct2   # model2 \n",
    "        index3 = ~(index1 | index2)    \n",
    "\n",
    "        # ----------------------\n",
    "        # model1 update\n",
    "        # ----------------------\n",
    "        loss1 = 0.0\n",
    "        if index2.any():\n",
    "            loss1 += kd_loss(\n",
    "                logits1[index2],\n",
    "                logits2[index2].detach(),\n",
    "                T=T\n",
    "            )\n",
    "        if index3.any():\n",
    "            loss1 += ce_loss(\n",
    "                logits1[index3],\n",
    "                y[index3]\n",
    "            )\n",
    "\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        # ----------------------\n",
    "        # model2 update\n",
    "        # ----------------------\n",
    "        loss2 = 0.0\n",
    "        if index1.any():\n",
    "            loss2 += kd_loss(\n",
    "                logits2[index1],\n",
    "                logits1[index1].detach(),\n",
    "                T=T\n",
    "            )\n",
    "        if index3.any():\n",
    "            loss2 += ce_loss(\n",
    "                logits2[index3],\n",
    "                y[index3]\n",
    "            )\n",
    "\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        train_loss1 += loss1.item()\n",
    "        train_loss2 += loss2.item()\n",
    "\n",
    "    return (\n",
    "        model1,\n",
    "        model2,\n",
    "        copy.deepcopy(model1.state_dict()),\n",
    "        copy.deepcopy(model2.state_dict()),\n",
    "        train_loss1 / len(train_loader),\n",
    "        train_loss2 / len(train_loader)\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee30a1b-c96a-471e-8651-25f4bc55dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 500\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_master = CNN_MNIST().to(device)\n",
    "mlp_master = MLP_MNIST().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0b1e05-9a9a-4758-8aa5-4e9d140f4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, model_type=\"CNN\", epochs=500):\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader = get_dataloaders(\n",
    "        train_dataset_small,\n",
    "        test_dataset,\n",
    "        seed\n",
    "    )\n",
    "\n",
    "    if model_type == \"CNN\":\n",
    "        model = copy.deepcopy(cnn_master).to(device)\n",
    "    elif model_type == \"MLP\":\n",
    "        model = copy.deepcopy(mlp_master).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_curve = []\n",
    "    acc_curve = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, criterion, train_loader, device\n",
    "        )\n",
    "\n",
    "        acc = evaluate_accuracy(model, test_loader, device)\n",
    "\n",
    "        train_curve.append(train_loss)\n",
    "        acc_curve.append(acc)\n",
    "        best_acc = max(best_acc, acc)\n",
    "\n",
    "    return {\n",
    "        \"best\": best_acc,\n",
    "        \"train_curve\": np.array(train_curve),\n",
    "        \"test_curve\": np.array(acc_curve)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0b641-2029-43ee-a867-7d8acd11fb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee89e2dd60aa44519bb817afa36de139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f425e416084245b103f32cefa00ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geunseopark/anaconda3/envs/PGS_TORCH/lib/python3.12/site-packages/torch/autograd/graph.py:841: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:304.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "seeds = [0, 1, 2, 3, 4]\n",
    "results_CNN = []\n",
    "results_MLP = []\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    out = run_experiment(seed, \"CNN\")\n",
    "    results_CNN.append(out)\n",
    "    out = run_experiment(seed, \"MLP\")\n",
    "    results_MLP.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0bd79-7271-4318-991a-f647e9730ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_ssmo(seed, model1_type=\"CNN\", model2_type=\"CNN\", epochs=500):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader = get_dataloaders(\n",
    "        train_dataset_small,\n",
    "        test_dataset,\n",
    "        seed\n",
    "    )\n",
    "\n",
    "    if model1_type == \"CNN\":\n",
    "        model1 = copy.deepcopy(cnn_master).to(device)\n",
    "    elif model1_type == \"MLP\":\n",
    "        model1 = copy.deepcopy(mlp_master).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model1\")\n",
    "\n",
    "    if model2_type == \"CNN\":\n",
    "        model2 = copy.deepcopy(cnn_master).to(device)\n",
    "    elif model2_type == \"MLP\":\n",
    "        model2 = copy.deepcopy(mlp_master).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model2\")\n",
    "\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_curve1 = []\n",
    "    acc_curve1 = []\n",
    "    best_acc1 = 0.0\n",
    "    train_curve2 = []\n",
    "    acc_curve2 = []\n",
    "    best_acc2 = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model1, model2, _, _, train_loss1, train_loss2 = studygroup(model1, model2, optimizer1, optimizer2, train_loader, device, T=2.0)\n",
    "        \n",
    "        acc1 = evaluate_accuracy(model1, test_loader, device)\n",
    "        acc2 = evaluate_accuracy(model2, test_loader, device)\n",
    "\n",
    "        train_curve1.append(train_loss1)\n",
    "        acc_curve1.append(acc1)\n",
    "        best_acc1 = max(best_acc1, acc1)\n",
    "        \n",
    "        train_curve2.append(train_loss2)\n",
    "        acc_curve2.append(acc2)\n",
    "        best_acc2 = max(best_acc2, acc2)\n",
    "\n",
    "    return {\n",
    "        \"best1\": best_acc1,\n",
    "        \"train_curve1\": np.array(train_curve1),\n",
    "        \"test_curve1\":  np.array(acc_curve1),\n",
    "        \"best2\": best_acc2,\n",
    "        \"train_curve2\": np.array(train_curve2),\n",
    "        \"test_curve2\":  np.array(acc_curve2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a56e3d-7fbc-44a7-aa36-8dbd233e2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 2, 3, 4]\n",
    "results_CNN_CNN = []\n",
    "results_MLP_CNN = []\n",
    "results_MLP_MLP = []\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"CNN\", model2_type=\"CNN\", epochs=500)\n",
    "    results_CNN_CNN.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"MLP\", model2_type=\"CNN\", epochs=500)\n",
    "    results_MLP_CNN.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"MLP\", model2_type=\"MLP\", epochs=500)\n",
    "    results_MLP_MLP.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f2994-9524-4b3f-ac2b-a326601a8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('results/MNIST/experiment_results_cnn.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_CNN)\n",
    "\n",
    "with open('results/MNIST/experiment_results_mlp.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_MLP)\n",
    "\n",
    "with open('results/MNIST/experiment_results_cnn_cnn.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_CNN_CNN)\n",
    "\n",
    "with open('results/MNIST/experiment_results_mlp_cnn.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_MLP_CNN)\n",
    "\n",
    "with open('results/MNIST/experiment_results_mlp_mlp.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_MLP_MLP)\n",
    "\n",
    "print(\"CSV 파일 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0f4d6-5f01-4b34-a219-aa38101d83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP-CNN Result\n",
    "\n",
    "test_curves1 = np.stack([r[\"test_curve\"] for r in results_CNN], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve1 = test_curves1.mean(axis=0)\n",
    "std_curve1 = test_curves1.std(axis=0)\n",
    "\n",
    "best_mean1 = np.mean([r[\"best\"] for r in results_CNN])\n",
    "best_std1 = np.std([r[\"best\"] for r in results_CNN])\n",
    "\n",
    "test_curves2 = np.stack([r[\"test_curve\"] for r in results_MLP], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve2 = test_curves2.mean(axis=0)\n",
    "std_curve2 = test_curves2.std(axis=0)\n",
    "\n",
    "best_mean2 = np.mean([r[\"best\"] for r in results_MLP])\n",
    "best_std2 = np.std([r[\"best\"] for r in results_MLP])\n",
    "\n",
    "test_curves3 = np.stack([r[\"test_curve1\"] for r in results_MLP_CNN], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve3 = test_curves3.mean(axis=0)\n",
    "std_curve3 = test_curves3.std(axis=0)\n",
    "\n",
    "best_mean3 = np.mean([r[\"best1\"] for r in results_MLP_CNN])\n",
    "best_std3 = np.std([r[\"best1\"] for r in results_MLP_CNN])\n",
    "\n",
    "test_curves4 = np.stack([r[\"test_curve2\"] for r in results_MLP_CNN], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve4 = test_curves4.mean(axis=0)\n",
    "std_curve4 = test_curves4.std(axis=0)\n",
    "\n",
    "best_mean4 = np.mean([r[\"best2\"] for r in results_MLP_CNN])\n",
    "best_std4 = np.std([r[\"best2\"] for r in results_MLP_CNN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995b1ad-1fba-4c9d-9ba7-18f44df8073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CNN mean: {best_mean1}\")\n",
    "print(f\"CNN std: {best_std1}\")\n",
    "print(f\"MLP mean: {best_mean2}\")\n",
    "print(f\"MLP std: {best_std2}\")\n",
    "print(f\"CNN_Ours mean: {best_mean4}\")\n",
    "print(f\"CNN_Ours std: {best_std4}\")\n",
    "print(f\"MLP_Ours mean: {best_mean3}\")\n",
    "print(f\"MLP_Ours std: {best_std3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c8328-b74a-45df-8a28-4ff4172eacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(len(mean_curve1))\n",
    "\n",
    "plt.plot(epochs[:], mean_curve1[:], label=\"CNN\")\n",
    "plt.fill_between(\n",
    "    epochs[:],\n",
    "    mean_curve1[:] - std_curve1[:],\n",
    "    mean_curve1[:] + std_curve1[:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[:], mean_curve2[:], label=\"MLP\")\n",
    "plt.fill_between(\n",
    "    epochs[:],\n",
    "    mean_curve2[:] - std_curve2[:],\n",
    "    mean_curve2[:] + std_curve2[:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[:], mean_curve4[:], label=\"CNN_Ours\")\n",
    "plt.fill_between(\n",
    "    epochs[:],\n",
    "    mean_curve4[:] - std_curve4[:],\n",
    "    mean_curve4[:] + std_curve4[:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[:], mean_curve3[:], label=\"MLP_Ours\")\n",
    "plt.fill_between(\n",
    "    epochs[:],\n",
    "    mean_curve3[:] - std_curve3[:],\n",
    "    mean_curve3[:] + std_curve3[:],\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecd77b-a917-4deb-85c7-9b261984ce46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
