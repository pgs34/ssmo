{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2bc2ae-a20b-4e4b-a837-09af7371ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ceb31f-9daf-4a17-8e39-acd480659bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cuDNN (중요)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # PyTorch 2.x (있으면 더 강력)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583bfb67-e74a-4dd2-b3da-080a9c10add6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "from neuralop.models import FNO2d\n",
    "from neuralop.data.datasets import DarcyDataset, NavierStokesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1.데이터 불러오기\n",
    "def split_dataset(dataset, split_ratio=0.8, seed=0):\n",
    "    N = len(dataset)\n",
    "    indices = torch.randperm(N, generator=torch.Generator().manual_seed(seed))\n",
    "    n_train = int(split_ratio * N)\n",
    "\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:]\n",
    "\n",
    "    return Subset(dataset, train_idx), Subset(dataset, test_idx)\n",
    "\n",
    "def subsample_dataset(dataset, n_samples, seed=0):\n",
    "    N = len(dataset)\n",
    "    assert n_samples <= N\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    indices = torch.randperm(N, generator=g)[:n_samples]\n",
    "\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "def get_dataset_loader(dataset_name, batch_size, seed=0):\n",
    "    if dataset_name == \"DarcyFlow\":\n",
    "        dataset = DarcyDataset(\n",
    "            root_dir='./data/darcy',\n",
    "            n_train=1000, n_tests=[100],\n",
    "            batch_size=batch_size, test_batch_sizes=[batch_size],\n",
    "            train_resolution=64, test_resolutions=[64],\n",
    "            download=True\n",
    "        )\n",
    "        \n",
    "        train_set, shifted_test_set = split_dataset(\n",
    "            dataset.train_db, split_ratio=0.8, seed=seed\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(shifted_test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    elif dataset_name == \"NavierStokes\":\n",
    "        dataset = NavierStokesDataset(\n",
    "            root_dir='./data/navier_stokes',\n",
    "            n_train=1000, n_tests=[100],\n",
    "            batch_size=batch_size, test_batch_sizes=[batch_size],\n",
    "            train_resolution=128, test_resolutions=[128],\n",
    "            download=True\n",
    "        )\n",
    "        train_loader = DataLoader(dataset.train_db, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(dataset.test_dbs[128], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    elif dataset_name == \"CarCFD\":\n",
    "        dataset = CarCFDDataset(\n",
    "            root_dir='./data/car_cfd/processed-car-pressure-data/',\n",
    "            n_train=1000, n_test=200,\n",
    "            query_res=[32, 32, 32],\n",
    "            download=False\n",
    "        )\n",
    "        train_loader = dataset.train_loader\n",
    "        test_loader = dataset.test_loader\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported yet.\")\n",
    "\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    in_channels = sample_batch[\"x\"].shape[1]\n",
    "    out_channels = sample_batch[\"y\"].shape[1]\n",
    "\n",
    "    return train_loader, test_loader, in_channels, out_channels\n",
    "\n",
    "# 2. Fine-tuning 함수\n",
    "def fine_tune(model, optimizer, criterion, train_loader, lr, device):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch[\"x\"].to(device), batch[\"y\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return model, copy.deepcopy(model.state_dict()), train_loss / len(train_loader)\n",
    "\n",
    "def studygroup(model1, model2, optimizer1, optimizer2, criterion, train_loader, lr, device):\n",
    "    model1.train()\n",
    "    train_loss1 = 0.0\n",
    "    model2.train()\n",
    "    train_loss2 = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch[\"x\"].to(device), batch[\"y\"].to(device)\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        outputs1 = model1(inputs)\n",
    "        outputs2 = model2(inputs)\n",
    "        loss1 = criterion(outputs1, targets)\n",
    "        loss2 = criterion(outputs2, targets)\n",
    "        \n",
    "        gap = torch.abs(outputs1 - targets) > torch.abs(outputs2 - targets)\n",
    "        sign = torch.sign(outputs1 - targets) != torch.sign(outputs2 - targets)\n",
    "        index1 = torch.where(torch.logical_and(gap, sign))\n",
    "        index2 = torch.where(torch.logical_and(~gap, sign))\n",
    "        index3 = torch.where(~sign)\n",
    "        \n",
    "        if index1[0].numel() > 0:\n",
    "            loss1 = criterion(outputs1[index1[0], index1[1], index1[2], index1[3]], outputs2[index1[0], index1[1], index1[2], index1[3]].detach())\n",
    "        if index3[0].numel() > 0:\n",
    "            loss1 += criterion(outputs1[index3[0], index3[1], index3[2], index3[3]], targets[index3[0], index3[1], index3[2], index3[3]])\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        if index2[0].numel() > 0:\n",
    "            loss2 = criterion(outputs2[index2[0], index2[1], index2[2], index2[3]], outputs1[index2[0], index2[1], index2[2], index2[3]].detach())\n",
    "        if index3[0].numel() > 0:\n",
    "            loss2 += criterion(outputs2[index3[0], index3[1], index3[2], index3[3]], targets[index3[0], index3[1], index3[2], index3[3]])        \n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        train_loss1 += loss1.item()\n",
    "        train_loss2 += loss2.item()\n",
    "        \n",
    "    return model1, model2, copy.deepcopy(model1.state_dict()), copy.deepcopy(model2.state_dict()), train_loss1 / len(train_loader), train_loss2 / len(train_loader)\n",
    "\n",
    "# 7. 평가 함수\n",
    "def evaluate(model, test_loader, device):\n",
    "    # criterion = nn.MSELoss()\n",
    "    # model.eval()\n",
    "    # test_loss = 0.0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch in test_loader:\n",
    "    #         inputs, targets = batch[\"x\"].to(device), batch[\"y\"].to(device)\n",
    "    #         outputs = model(inputs)\n",
    "    #         test_loss += criterion(outputs, targets).item()\n",
    "    # return test_loss / len(test_loader)\n",
    "    model.eval()\n",
    "    total_rel_l2 = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"x\"].to(device)\n",
    "            targets = batch[\"y\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            diff = outputs - targets\n",
    "\n",
    "            diff_norm = torch.norm(diff.view(diff.size(0), -1), dim=1)\n",
    "            target_norm = torch.norm(targets.view(targets.size(0), -1), dim=1)\n",
    "\n",
    "            rel_l2 = diff_norm / (target_norm + 1e-12)\n",
    "            total_rel_l2 += rel_l2.sum().item()\n",
    "            count += rel_l2.size(0)\n",
    "\n",
    "    return total_rel_l2 / count\n",
    "\n",
    "class DeepONetDarcyWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, H, W):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "\n",
    "        x = torch.linspace(0, 1, W)\n",
    "        y = torch.linspace(0, 1, H)\n",
    "        grid = torch.stack(torch.meshgrid(x, y, indexing=\"xy\"), dim=-1)\n",
    "        self.register_buffer(\"coords\", grid.reshape(-1, 2))\n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        a: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        B = a.size(0)\n",
    "        branch = a.view(B, -1)\n",
    "        trunk = self.coords.unsqueeze(0).repeat(B, 1, 1)\n",
    "        out = self.model((branch, trunk))\n",
    "        return out.view(B, 1, self.H, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185788c5-f9d6-4811-a9f6-f06972928753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.fno import FNO2D\n",
    "from models.deeponet import DeepONet2D\n",
    "from models.gnot import GNOT2D\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# FNO 모델 초기화\n",
    "pretrained_FNO = FNO2D(in_ch=1, out_ch=1, width=96, modes=12, depth=4).to(device)\n",
    "pretrained_DON = DeepONetDarcyWrapper(DeepONet2D(branch_dim=64*64, trunk_dim=2, hidden=128, depth=3).to(device), 64, 64).to(device)\n",
    "pretrained_GNOT = GNOT2D(in_channels=1, out_channels=1, width=64, latent_dim=64, n_latents=64, depth=4, heads=4).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0569036-f6bd-415f-939a-f7fa6214caf4",
   "metadata": {},
   "source": [
    "print(pretrained_model1)\n",
    "print(pretrained_model3)\n",
    "print(pretrained_model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51cde869-e076-4643-b37e-6ef7cd1df48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, model_type=\"FNO\", epochs=1000):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader, _, _ = get_dataset_loader(\n",
    "        \"DarcyFlow\", batch_size=16, seed=seed\n",
    "    )\n",
    "\n",
    "    if model_type == \"FNO\":\n",
    "        model = copy.deepcopy(pretrained_FNO).to(device)\n",
    "    elif model_type == \"DeepONet\":\n",
    "        model = copy.deepcopy(pretrained_DON).to(device)\n",
    "    elif model_type == \"GNOT\":\n",
    "        model = copy.deepcopy(pretrained_GNOT).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1e-3, weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_curve = []\n",
    "    test_curve  = []\n",
    "    best_test = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model, _, train_loss = fine_tune(\n",
    "            model, optimizer, criterion,\n",
    "            train_loader, lr=1e-3, device=device\n",
    "        )\n",
    "\n",
    "        test_error = evaluate(model, test_loader, device)\n",
    "\n",
    "        train_curve.append(train_loss)\n",
    "        test_curve.append(test_error)\n",
    "\n",
    "        best_test = min(best_test, test_error)\n",
    "\n",
    "    return {\n",
    "        \"best\": best_test,\n",
    "        \"train_curve\": np.array(train_curve),\n",
    "        \"test_curve\":  np.array(test_curve),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17522bbd-5710-49f4-8311-8ebb55426c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec02eaf3ffde41f9bed9f6f1a8899af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 64 with 100 samples \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac57e43f7a14fe79e38593f1105db2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 64 with 100 samples \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1823e679904f279043c8ce02f2d2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 64 with 100 samples \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ad9a80fe154401bd072474a8805090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seeds = [0, 1, 2, 3, 4]\n",
    "results_FNO = []\n",
    "results_DON = []\n",
    "results_GNOT = []\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    out = run_experiment(seed, model_type=\"FNO\", epochs=1000)\n",
    "    results_FNO.append(out)\n",
    "    out = run_experiment(seed, model_type=\"DeepONet\", epochs=1000)\n",
    "    results_DON.append(out)\n",
    "    out = run_experiment(seed, model_type=\"GNOT\", epochs=1000)\n",
    "    results_GNOT.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392913e6-74ed-4d68-923c-05ab4606d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_ssmo(seed, model1_type=\"FNO\", model2_type=\"FNO\", epochs=1000):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader, _, _ = get_dataset_loader(\n",
    "        \"DarcyFlow\", batch_size=16, seed=seed\n",
    "    )\n",
    "\n",
    "    if model1_type == \"FNO\":\n",
    "        model1 = copy.deepcopy(pretrained_FNO).to(device)\n",
    "    elif model1_type == \"DeepONet\":\n",
    "        model1 = copy.deepcopy(pretrained_DON).to(device)\n",
    "    elif model1_type == \"GNOT\":\n",
    "        model1 = copy.deepcopy(pretrained_GNOT).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model1\")\n",
    "\n",
    "    if model2_type == \"FNO\":\n",
    "        model2 = copy.deepcopy(pretrained_FNO).to(device)\n",
    "    elif model2_type == \"DeepONet\":\n",
    "        model2 = copy.deepcopy(pretrained_DON).to(device)\n",
    "    elif model2_type == \"GNOT\":\n",
    "        model2 = copy.deepcopy(pretrained_GNOT).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model2\")\n",
    "\n",
    "    optimizer1 = torch.optim.Adam(\n",
    "        model1.parameters(), lr=1e-3, weight_decay=1e-4\n",
    "    )\n",
    "    optimizer2 = torch.optim.Adam(\n",
    "        model2.parameters(), lr=1e-3, weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_curve1 = []\n",
    "    test_curve1 = []\n",
    "    best_test1 = float(\"inf\")\n",
    "    train_curve2 = []\n",
    "    test_curve2 = []\n",
    "    best_test2 = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model1, model2, _, _, train_loss1, train_loss2 = studygroup(model1, model2, optimizer1, optimizer2, criterion, train_loader, lr=1e-3, device=device)\n",
    "\n",
    "        test_error1 = evaluate(model1, test_loader, device)\n",
    "        test_error2 = evaluate(model2, test_loader, device)\n",
    "\n",
    "        train_curve1.append(train_loss1)\n",
    "        test_curve1.append(test_error1)\n",
    "        train_curve2.append(train_loss2)\n",
    "        test_curve2.append(test_error2)\n",
    "\n",
    "        best_test1 = min(best_test1, test_error1)\n",
    "        best_test2 = min(best_test2, test_error2)\n",
    "\n",
    "    return {\n",
    "        \"best1\": best_test1,\n",
    "        \"train_curve1\": np.array(train_curve1),\n",
    "        \"test_curve1\":  np.array(test_curve1),\n",
    "        \"best2\": best_test2,\n",
    "        \"train_curve2\": np.array(train_curve2),\n",
    "        \"test_curve2\":  np.array(test_curve2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ad613-661f-4feb-b2d4-fc89fb685b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 2, 3, 4]\n",
    "results_FNO_FNO = []\n",
    "results_DON_FNO = []\n",
    "results_GNOT_FNO = []\n",
    "results_DON_DON = []\n",
    "results_GNOT_DON = []\n",
    "results_GNOT_GNOT = []\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"FNO\", model2_type=\"FNO\", epochs=1000)\n",
    "    results_FNO_FNO.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"DeepONet\", model2_type=\"FNO\", epochs=1000)\n",
    "    results_DON_FNO.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"GNOT\", model2_type=\"FNO\", epochs=1000)\n",
    "    results_GNOT_FNO.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"DeepONet\", model2_type=\"DeepONet\", epochs=1000)\n",
    "    results_DON_DON.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"GNOT\", model2_type=\"DeepONet\", epochs=1000)\n",
    "    results_GNOT_DON.append(out)\n",
    "    out = run_experiment_ssmo(seed, model1_type=\"GNOT\", model2_type=\"GNOT\", epochs=1000)\n",
    "    results_GNOT_GNOT.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c2048-e732-47b6-be43-2ebd4d10f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('results/darcy2d/experiment_results_fno.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_FNO)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_deeponet.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_DON)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_gnot.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_GNOT)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_fno_fno.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_FNO_FNO)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_deeponet_fno.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_DON_FNO)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_gnot_fno.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_GNOT_FNO)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_deeponet_deeponet.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_DON_DON)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_deeponet_gnot.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_GNOT_DON)\n",
    "\n",
    "with open('results/darcy2d/experiment_results_gnot_gnot.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_GNOT_GNOT)\n",
    "\n",
    "print(\"CSV 파일 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c13439-b333-4186-8487-b6ca0527920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepONet-FNO Result\n",
    "\n",
    "test_curves1 = np.stack([r[\"test_curve\"] for r in results_FNO], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve1 = test_curves1.mean(axis=0)\n",
    "std_curve1 = test_curves1.std(axis=0)\n",
    "\n",
    "best_mean1 = np.mean([r[\"best\"] for r in results_FNO])\n",
    "best_std1 = np.std([r[\"best\"] for r in results_FNO])\n",
    "\n",
    "test_curves2 = np.stack([r[\"test_curve\"] for r in results_DON], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve2 = test_curves2.mean(axis=0)\n",
    "std_curve2 = test_curves2.std(axis=0)\n",
    "\n",
    "best_mean2 = np.mean([r[\"best\"] for r in results_DON])\n",
    "best_std2 = np.std([r[\"best\"] for r in results_DON])\n",
    "\n",
    "test_curves3 = np.stack([r[\"test_curve1\"] for r in results_DON_FNO], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve3 = test_curves3.mean(axis=0)\n",
    "std_curve3 = test_curves3.std(axis=0)\n",
    "\n",
    "best_mean3 = np.mean([r[\"best1\"] for r in results_DON_FNO])\n",
    "best_std3 = np.std([r[\"best1\"] for r in results_DON_FNO])\n",
    "\n",
    "test_curves4 = np.stack([r[\"test_curve2\"] for r in results_DON_FNO], axis=0)\n",
    "# shape: (num_seeds, epochs)\n",
    "\n",
    "mean_curve4 = test_curves4.mean(axis=0)\n",
    "std_curve4 = test_curves4.std(axis=0)\n",
    "\n",
    "best_mean4 = np.mean([r[\"best2\"] for r in results_DON_FNO])\n",
    "best_std4 = np.std([r[\"best2\"] for r in results_DON_FNO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6dae-80bf-4a82-872c-ef2a2e3a6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FNO mean: {best_mean1}\")\n",
    "print(f\"FNO std: {best_std1}\")\n",
    "print(f\"DeepONet mean: {best_mean2}\")\n",
    "print(f\"DeepONet std: {best_std2}\")\n",
    "print(f\"FNO_Ours mean: {best_mean4}\")\n",
    "print(f\"FNO_Ours std: {best_std4}\")\n",
    "print(f\"DeepONet_Ours mean: {best_mean3}\")\n",
    "print(f\"DeepONet_Ours std: {best_std3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf77735-a4e2-42be-9458-fd4a03f85ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(len(mean_curve1))\n",
    "\n",
    "plt.plot(epochs[200:], mean_curve1[200:], label=\"FNO\")\n",
    "plt.fill_between(\n",
    "    epochs[200:],\n",
    "    mean_curve1[200:] - std_curve1[200:],\n",
    "    mean_curve1[200:] + std_curve1[200:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[200:], mean_curve2[200:], label=\"DeepONet\")\n",
    "plt.fill_between(\n",
    "    epochs[200:],\n",
    "    mean_curve2[200:] - std_curve2[200:],\n",
    "    mean_curve2[200:] + std_curve2[200:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[200:], mean_curve4[200:], label=\"FNO_Ours\")\n",
    "plt.fill_between(\n",
    "    epochs[200:],\n",
    "    mean_curve4[200:] - std_curve4[200:],\n",
    "    mean_curve4[200:] + std_curve4[200:],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.plot(epochs[200:], mean_curve3[200:], label=\"DeepONet_Ours\")\n",
    "plt.fill_between(\n",
    "    epochs[200:],\n",
    "    mean_curve3[200:] - std_curve3[200:],\n",
    "    mean_curve3[200:] + std_curve3[200:],\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Relative L2 Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
